{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4bc3b7",
   "metadata": {},
   "source": [
    "## **Description**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f1936",
   "metadata": {},
   "source": [
    "This script is a semi-automated script for subject inclusion/exclusion, preprocessing and ΔERN value extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd7d48",
   "metadata": {},
   "source": [
    "### Modules and packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03d97d",
   "metadata": {},
   "source": [
    "- Python version: 3.15.5  \n",
    "- Required packages:\n",
    "    - `mne`\n",
    "    - `autoreject`\n",
    "    - `asrpy`\n",
    "    - `matplotlib`\n",
    "    - `pandas`\n",
    "    - `numpy`\n",
    "    - `csv` \n",
    "    - `os` \n",
    "- Run the cell below for all imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf28af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne import preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from autoreject import AutoReject\n",
    "import csv \n",
    "from mne.preprocessing import peak_finder\n",
    "import pandas as pd\n",
    "from mne import Annotations\n",
    "import asrpy\n",
    "import time \n",
    "from mne.time_frequency import psd_array_welch\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159a517",
   "metadata": {},
   "source": [
    "### Data BIDS structure\n",
    "\n",
    "```text\n",
    "project_root\n",
    "├── rawdata\n",
    "│   └── sub-AG04EN28\n",
    "│       └── eeg\n",
    "│           ├── sub-AG04EN28_task-ern_eeg.set\n",
    "│           ├── sub-AG04EN28_task-ern_channels.tsv\n",
    "│           └── …\n",
    "├── derivatives\n",
    "│   └── ernpipeline\n",
    "│       ├── plots (For subject-specific ERN plots)\n",
    "│       │   ├── sub-AG04EN28_plot.png\n",
    "│       │   ├── sub-CK06ST11_plot.png\n",
    "│       │   └── …\n",
    "│       ├── sub-AG04EN28\n",
    "│       │   ├── sub-AG04EN28_desc-epochs_eeg.fif\n",
    "│       │   ├── sub-AG04EN28_desc-evokedCorrect_eeg.fif\n",
    "│       │   ├── sub-AG04EN28_desc-evokedIncorrect_eeg.fif\n",
    "│       │   ├── sub-AG04EN28_desc-preprocSummary.txt\n",
    "│       │   └── sub-AG04EN28_desc-ernValues.tsv\n",
    "│       └── …\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da77d0",
   "metadata": {},
   "source": [
    "The scripts follows a BIDS valid organization and syntax. It creates all the necessary folders for the outputs in your project directory folder. The only path to update will be the root_path to the project directory below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094c510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Setting the paths to fit the BIDS requirements ########\n",
    "\n",
    "pipeline_name = \"ernpipeline\"  # change this if needed\n",
    "\n",
    "# Set the root path to your project directory !!\n",
    "root_path = \"C:/Users/qmoreau/Documents/Work/EEGManyAnalysts\"\n",
    "\n",
    "# Base directories\n",
    "paths = {\n",
    "    \"raw\": os.path.join(root_path, \"raw\"),\n",
    "    \"deriv\": os.path.join(root_path, \"derivatives\", pipeline_name),\n",
    "    \"plots\": os.path.join(root_path, \"derivatives\", pipeline_name, \"plots\"),\n",
    "    \"results\": os.path.join(root_path, \"results\")\n",
    "}\n",
    "\n",
    "os.makedirs(paths[\"plots\"], exist_ok=True)\n",
    "os.makedirs(paths[\"results\"], exist_ok=True)\n",
    "\n",
    "# Utility functions to construct paths (BIDS compliant - adjust if necessary)\n",
    "\n",
    "def raw_set_path(participant):\n",
    "    return os.path.join(paths[\"raw\"], participant, \"eeg\", f\"{participant}_task-Flanker_eeg.set\")\n",
    "\n",
    "def events_json_path(participant):\n",
    "    return os.path.join(paths[\"raw\"], participant, \"eeg\", f\"{participant}_task-Flanker_events.json\")\n",
    "\n",
    "def participant_dir(participant):\n",
    "    return os.path.join(paths[\"deriv\"], f\"{participant}\")\n",
    "\n",
    "def epochs_file(participant):\n",
    "    return os.path.join(participant_dir(participant), f\"{participant}_desc-epochs_eeg.fif\")\n",
    "\n",
    "def evoked_file(participant, kind):  # kind in ['Correct', 'Incorrect', 'dif']\n",
    "    suffix = {\n",
    "        'Correct': '_desc-evokedCorrect_eeg.fif',\n",
    "        'Incorrect': '_desc-evokedIncorrect_eeg.fif',\n",
    "        'dif': '_desc-evokedDiff_eeg.fif'\n",
    "    }[kind]\n",
    "    return os.path.join(participant_dir(participant), suffix.replace('-', f\"-{participant}-\"))\n",
    "\n",
    "def summary_txt_path(participant):\n",
    "    return os.path.join(participant_dir(participant), f\"{participant}_desc-preprocSummary.txt\")\n",
    "\n",
    "def ern_values_tsv(participant):\n",
    "    return os.path.join(participant_dir(participant), f\"{participant}_desc-ernValues.tsv\")\n",
    "\n",
    "def out_plot(participant):\n",
    "    return os.path.join(paths[\"plots\"], f\"{participant}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ec27d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-AL07NK06', 'sub-CH05NK28', 'sub-ER05RT01', 'sub-ER06OR21', 'sub-ER07RL02', 'sub-FF07NS13', 'sub-IR05ET18', 'sub-LS08LF22', 'sub-NG03YI17', 'sub-NN07EN14', 'sub-NN09EN10', 'sub-ON05EL22', 'sub-RI04ZA30', 'sub-RT09CH24', 'sub-SE05ZI24', 'sub-TI04NS30', 'sub-US08RI28', 'sub-VA06OV12', 'sub-XX04AS28']\n"
     ]
    }
   ],
   "source": [
    "######## Gather participantects from raw folder )\n",
    "# Only include directories starting with 'sub-'\n",
    "files = [d for d in os.listdir(paths[\"raw\"]) \\\n",
    "                if d.startswith(\"sub-\") and os.path.isdir(os.path.join(paths[\"raw\"], d))]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bd23e",
   "metadata": {},
   "source": [
    "## **Subject quality check for inclusion/exclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd4fd8",
   "metadata": {},
   "source": [
    "This section allows to perfrom an automatic quality control across all subjects using four quality metrics (OHA, THV, CHV, BCR) (Extracted from Automagic's pipeline [DOI:http://dx.doi.org/10.1101/460469doi]). A quality score is computed based on OHA, THV, and CHV, where higher scores indicate noisier data. Subjects with a quality score in the top 5% (i.e., above the 95th percentile) are excluded. Additionally, any subject with a Bad Channels Ratio (BCR) above 20% is also excluded.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4cfe8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qmoreau\\AppData\\Local\\Temp\\ipykernel_35216\\4104413271.py:39: RuntimeWarning: Estimated head radius (0.1 cm) is below the 3rd percentile for infant head size. Check if the montage_units argument is correct (the default is \"mm\", but your channel positions may be in different units).\n",
      "  raw = mne.io.read_raw_eeglab(raw_set_path(participant), preload=True, verbose=False)\n",
      "C:\\Users\\qmoreau\\AppData\\Local\\Temp\\ipykernel_35216\\4104413271.py:39: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(raw_set_path(participant), preload=True, verbose=False)\n",
      "C:\\Users\\qmoreau\\AppData\\Local\\Temp\\ipykernel_35216\\4104413271.py:39: RuntimeWarning: Not setting position of 1 eog channel found in montage:\n",
      "['VOGbelow']\n",
      "Consider setting the channel types to be of EEG/sEEG/ECoG/DBS/fNIRS using inst.set_channel_types before calling inst.set_montage, or omit these channels when creating your montage.\n",
      "  raw = mne.io.read_raw_eeglab(raw_set_path(participant), preload=True, verbose=False)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'mne.preprocessing' has no attribute 'find_bad_channels_lof'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m raw \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mdrop_channels(unused_channels)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# automatically detect bad channels using Local Outlier Factof (LOF), interpolate and keep them\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m bad_channels \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mfind_bad_channels_lof(raw, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, picks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, return_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m bad_channels_raw \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m bad_channels:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mne.preprocessing' has no attribute 'find_bad_channels_lof'"
     ]
    }
   ],
   "source": [
    "subject_list = files\n",
    "\n",
    "######## Defining a function that computes our 3 metrics of interest\n",
    "def compute_oha_thv_chv(raw, theta_oha, theta_thv, theta_chv, window_s=1.0, step_s=0.5):\n",
    "    \n",
    "    # Extraction EEG data (n_chan × n_times)\n",
    "    picks = mne.pick_types(raw.info, eeg=True, meg=False)\n",
    "    data = raw.get_data(picks=picks)             \n",
    "    sfreq = raw.info['sfreq']\n",
    "\n",
    "    # OHA : Overall High Amplitude\n",
    "    oha = np.mean(np.abs(data) > theta_oha)          \n",
    "\n",
    "    # THV : Timepoints of high variance\n",
    "    w = int(window_s * sfreq)\n",
    "    st = int(step_s  * sfreq)\n",
    "    n_win = n_bad = 0\n",
    "    for start in range(0, data.shape[1] - w + 1, st):\n",
    "        seg = data[:, start:start + w]               \n",
    "        n_win += 1\n",
    "        std_ch_t = np.nanstd(seg, axis=0)          \n",
    "        if np.any(std_ch_t > theta_thv):\n",
    "            n_bad += 1\n",
    "    thv = n_bad / n_win if n_win > 0 else np.nan\n",
    "\n",
    "    # CHV : Channels of high variance\n",
    "    chan_sd = np.nanstd(data, axis=1)                \n",
    "    chv = np.mean(chan_sd > theta_chv)              \n",
    "\n",
    "    return oha, thv, chv\n",
    "\n",
    "qc_rows = []\n",
    "\n",
    "######## Loop on all subjects \n",
    "\n",
    "for participant in subject_list : \n",
    "    \n",
    "    # load subject\n",
    "    raw = mne.io.read_raw_eeglab(raw_set_path(participant), preload=True, verbose=False)\n",
    "    unused_channels = ['MASTl','MASTr']\n",
    "    raw = raw.drop_channels(unused_channels)\n",
    "  \n",
    "    # automatically detect bad channels using Local Outlier Factof (LOF), interpolate and keep them\n",
    "    bad_channels = mne.preprocessing.find_bad_channels_lof(raw, n_neighbors=20, picks=None, metric=\"euclidean\", threshold=1.5, return_scores=False, verbose=None)\n",
    "    bad_channels_raw = raw.copy()\n",
    "    for ch in bad_channels:\n",
    "        if ch not in raw.info[\"bads\"]:\n",
    "            bad_channels_raw.info[\"bads\"].append(ch)\n",
    "\n",
    "    # computing the ratio of bad channels to good channels (BCR)\n",
    "    n_bads = len(bad_channels_raw.info[\"bads\"])\n",
    "    n_eeg = len(mne.pick_types(raw.info, eeg=True))\n",
    "    n_goods = n_eeg - n_bads\n",
    "    BCR = n_bads / n_goods\n",
    "    \n",
    "    # Rereferencing to average \n",
    "    bad_channels_raw.set_eeg_reference('average')\n",
    "\n",
    "    # filtering \n",
    "    bad_channels_raw.filter(l_freq=.1, h_freq=30)\n",
    "\n",
    "    # compute OHA at 30µV\n",
    "    oha30, _, _ = compute_oha_thv_chv(bad_channels_raw, theta_oha=30e-6, theta_thv=30e-6, theta_chv=30e-6)\n",
    "    # compute THV and CHV  at 15µV\n",
    "    _, thv15, chv15 = compute_oha_thv_chv(bad_channels_raw, theta_oha=15e-6, theta_thv=15e-6, theta_chv=15e-6)\n",
    "\n",
    "    qc_rows.append({\n",
    "        'sub':    participant,\n",
    "        'OHA_30': oha30*100,\n",
    "        'THV_15': thv15*100,\n",
    "        'CHV_15': chv15*100,\n",
    "        'BCR' : BCR*100\n",
    "    })\n",
    "\n",
    "######## build DataFrame\n",
    "qc_df = pd.DataFrame(qc_rows)\n",
    "df = qc_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7a69f",
   "metadata": {},
   "source": [
    "The cell below allows to plot the number of subjects for each quality score value for data visualization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1567e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######## Computing quality score using adjustable weights \n",
    "\n",
    "# Weight definition for each metric (Adjust the weights)\n",
    "w = {\"OHA_30\":0.50, \"THV_15\":0.15, \"CHV_15\":0.30, \"BCR\":0.05}\n",
    "\n",
    "# Calculate quality score and add to dataframe \n",
    "df[\"Qscore\"]= (df[list(w)].mul(w).sum(axis=1))\n",
    "\n",
    "######## Plotting for visual inspection allowing wieghts and thresholds adjustment\n",
    "\n",
    "# 4d scattered plot (interactive)\n",
    "fig = px.scatter_3d(df, x=\"OHA_30\", y=\"THV_15\", z=\"CHV_15\", color=\"BCR\", hover_name=\"sub\", opacity=0.8)\n",
    "fig.update_layout(scene=dict(xaxis_title=\"OHA_30 (%)\", yaxis_title=\"THV_15 (%)\", zaxis_title=\"CHV_15 (%)\"), title=\"EEG QC space\")\n",
    "fig.show()\n",
    "\n",
    "# Distribution plot \n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df[\"Qscore\"], bins=30, kde=True)\n",
    "plt.xlabel(\"Qscore\")\n",
    "plt.title(\"Quality Score Distribution\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073be318",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Setting threshold at 5th percentile of Qscore distribution \n",
    "\n",
    "qscore_thresh = df['Qscore'].quantile(0.05)\n",
    "\n",
    "# Keep only subjects above the Qscore threshold\n",
    "df_filtered = df[df['Qscore'] >= qscore_thresh]\n",
    "\n",
    "# Identify subjects to exclude based on Qscore OR BCR ≥ 20\n",
    "exclude = df[(df['Qscore'] < qscore_thresh) | (df['BCR'] >= 20)]['sub'].tolist()\n",
    "\n",
    "# Save filtered dataframe (only subjects above threshold) to CSV\n",
    "df_filtered.to_csv(os.path.join(paths['deriv'], 'quality_control.csv'), index=False)\n",
    "\n",
    "# Print how many subjects were excluded\n",
    "print(\"Number of subjects excluded based on quality label and BCR threshold:\", len(exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983f024",
   "metadata": {},
   "source": [
    "## **Preprocessing and ERN analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bed671",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Defining used functions for the pipeline\n",
    "\n",
    "def compute_congruency_ERPs(epochs, congruency, participant, channel=\"CZ\", plot_dir=out_plot):\n",
    "        \"\"\"\n",
    "        Computes ERPs, Diff, mean_amp_diff, and plots + saves result for a given congruency level.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs : mne.Epochs\n",
    "        - congruency : str, one of [\"0\", \"33\", \"66\", \"100\", \"all\"]\n",
    "        - participant : str, participant ID for filenames\n",
    "        - channel : str\n",
    "        - plot_dir : str, folder to save plots. If None, no saving.\n",
    "\n",
    "        Returns:\n",
    "        - ERP_correct, ERP_incorrect, Diff, mean_amp_diff\n",
    "        \"\"\"\n",
    "        event_codes = {\n",
    "            \"0\":    {\"correct\": [\"106\", \"107\"], \"incorrect\": [\"108\", \"109\"]},\n",
    "            \"33\":   {\"correct\": [\"116\", \"117\"], \"incorrect\": [\"118\", \"119\"]},\n",
    "            \"66\":   {\"correct\": [\"126\", \"127\"], \"incorrect\": [\"128\", \"129\"]},\n",
    "            \"100\":  {\"correct\": [\"136\", \"137\"], \"incorrect\": [\"138\", \"139\"]},\n",
    "            \"all\":  {\"correct\": [\"106\", \"107\", \"116\", \"117\", \"126\", \"127\", \"136\", \"137\"], \"incorrect\": [\"108\", \"109\", \"118\", \"119\", \"128\", \"129\", \"138\", \"139\"]}\n",
    "        }\n",
    "\n",
    "        if congruency not in event_codes:\n",
    "            raise ValueError(f\"Invalid congruency: {congruency}\")\n",
    "\n",
    "        correct_ids = [e for e in event_codes[congruency][\"correct\"] if e in epochs.event_id]\n",
    "        incorrect_ids = [e for e in event_codes[congruency][\"incorrect\"] if e in epochs.event_id]\n",
    "\n",
    "        epochs_correct = epochs[correct_ids] if correct_ids else None\n",
    "        epochs_incorrect = epochs[incorrect_ids] if incorrect_ids else None\n",
    "\n",
    "        def safe_avg(e, name):\n",
    "            if e is not None and len(e) > 0:\n",
    "                evoked = e.average()\n",
    "                evoked.comment = name\n",
    "                return evoked\n",
    "            else:\n",
    "                print(f\"No epochs found for {name}\")\n",
    "                return None\n",
    "\n",
    "        ERP_correct = safe_avg(epochs_correct, f\"Correct {congruency}%\")\n",
    "        ERP_incorrect = safe_avg(epochs_incorrect, f\"Incorrect {congruency}%\")\n",
    "\n",
    "        mean_amp_diff = None\n",
    "        Diff = None\n",
    "\n",
    "        if ERP_correct and ERP_incorrect:\n",
    "            Diff = ERP_correct.copy()\n",
    "            Diff._data = ERP_incorrect._data - ERP_correct._data\n",
    "            Diff.comment = f\"Difference Wave {congruency}%\"\n",
    "\n",
    "            # Peak detection in the 0–0.5 s window\n",
    "            ch_idx = Diff.ch_names.index(channel)\n",
    "            data = Diff.data[ch_idx]\n",
    "            mask = (Diff.times >= 0) & (Diff.times <= 0.5)\n",
    "            data_win = data[mask]\n",
    "            times_win = Diff.times[mask]\n",
    "\n",
    "            peak_locs, peak_vals = mne.preprocessing.peak_finder(data_win, extrema=-1)\n",
    "\n",
    "            if len(peak_locs) > 0:\n",
    "                # Take the strongest negative peak (minimum value)\n",
    "                peak = np.argmin(peak_vals)\n",
    "                peak_time = times_win[peak_locs[peak]]\n",
    "                tmin_win = peak_time - 0.1\n",
    "                tmax_win = peak_time + 0.1\n",
    "                Diff_cropped = Diff.copy().pick(channel).crop(tmin_win, tmax_win)\n",
    "                mean_amp_diff = Diff_cropped.data.mean(axis=1) * 1e6  \n",
    "            else:\n",
    "                mean_amp_diff = None\n",
    "\n",
    "        # Plot & save\n",
    "        if ERP_correct and ERP_incorrect and Diff and plot_dir:\n",
    "            os.makedirs(plot_dir, exist_ok=True)\n",
    "            out_plot = os.path.join(plot_dir, f\"{participant}_cong-{congruency}_plot.png\")\n",
    "            fig = mne.viz.plot_compare_evokeds(\n",
    "                [ERP_correct, ERP_incorrect, Diff], picks=channel\n",
    "            )[0]\n",
    "            fig.savefig(out_plot, dpi=300)\n",
    "            print(f\"Saved plot for {congruency}% → {out_plot}\")\n",
    "\n",
    "        return ERP_correct, ERP_incorrect, Diff, mean_amp_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = files\n",
    "\n",
    "failed_subjects = []\n",
    "\n",
    "for participant in subject_list : \n",
    "    \n",
    "    # Define preprocessed epochs file\n",
    "    preproc_file = epochs_file(participant)\n",
    "    # Skip if QC excluded or already preprocessed\n",
    "    if participant in exclude or os.path.exists(preproc_file):\n",
    "        reason = \"excluded because too noisy :(\" if participant in exclude else \"already preprocessed :)!\"\n",
    "        print(f\"Subject {participant} {reason}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    #################\n",
    "    # PREPROCESSING #\n",
    "    #################\n",
    "    \n",
    "    print(f\"Processing subject {participant}...\")\n",
    "  \n",
    "    #load subject\n",
    "    raw = mne.io.read_raw_eeglab(raw_set_path(participant), preload=True, verbose=False)\n",
    "    unused_channels = ['MASTl','MASTr']\n",
    "    raw = raw.drop_channels(unused_channels)\n",
    "\n",
    "    #Adding HEOG as EOG\n",
    "    heog_data = raw.copy().pick_channels(['F7','F8']).get_data()\n",
    "    heog = heog_data[0] - heog_data[1]\n",
    "    heog_info = mne.create_info(['HEOG'], raw.info['sfreq'], 'eog')\n",
    "    heog_raw = mne.io.RawArray(heog[np.newaxis,:], heog_info)\n",
    "    raw_heog = raw.copy()\n",
    "    raw_heog = raw_heog.add_channels([heog_raw], force_update_info=True)\n",
    "  \n",
    "    # automatically detect bad channels using Local Outlier Factof (LOF), interpolate and keep them\n",
    "    bad_channels = mne.preprocessing.find_bad_channels_lof(raw_heog, n_neighbors=20, picks=None, metric=\"euclidean\", threshold=1.5, return_scores=False, verbose=None)\n",
    "    bad_channels_raw = raw_heog.copy()\n",
    "    for ch in bad_channels:\n",
    "        if ch not in raw.info[\"bads\"]:\n",
    "            bad_channels_raw.info[\"bads\"].append(ch)\n",
    "\n",
    "    # computing the ratio of bad channels to good channels (BCR)\n",
    "    n_bads = len(bad_channels_raw.info[\"bads\"])\n",
    "    n_eeg = len(mne.pick_types(raw.info, eeg=True))\n",
    "    n_goods = n_eeg - n_bads\n",
    "    BCR = n_bads / n_goods\n",
    "    \n",
    "    raw_interp = bad_channels_raw.copy().interpolate_bads()\n",
    "    \n",
    "    # Rereferencing to average \n",
    "    raw_interp.set_eeg_reference('average')\n",
    "    \n",
    "    # filtering to remove slow drifts\n",
    "    filt_raw = raw_interp.copy()\n",
    "    filt_raw.load_data().filter(l_freq=1., h_freq=None)\n",
    "    \n",
    "    # cleaning with artifact subspace reconstruction (ASR)\n",
    "    try : \n",
    "        eeg_picks = mne.pick_types(filt_raw.info, eeg=True)\n",
    "        asr = asrpy.ASR(sfreq=filt_raw.info['sfreq'], cutoff=15)\n",
    "        asr.fit(filt_raw.copy().pick(eeg_picks))\n",
    "        asr_raw = asr.transform(filt_raw.copy())\n",
    "    except Exception as e:\n",
    "        print(f\"ASR failed for {participant}: {e}, skipping...\")\n",
    "        # log failed particpants \n",
    "        failed_subjects.append(participant)\n",
    "        with open(os.path.join(paths['deriv'], \"asr_failed_subjects.txt\"), \"a\") as f:\n",
    "            f.write(f\"{participant}: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "    # VEOG: VOG_below and FP1 \n",
    "    raw_eog = asr_raw.copy()\n",
    "    raw_eog = mne.set_bipolar_reference(asr_raw, anode=\"VOGbelow\", cathode=\"FP1\", ch_name='VEOG', copy=False, drop_refs=False)\n",
    "    raw_eog.set_channel_types({\"VEOG\": 'eog'})\n",
    "    raw_eog.drop_channels(['VOGbelow'])\n",
    "\n",
    "    # ICA - Only computing the 25 first components for speed issue, using 42 as a random seed\n",
    "    ica = mne.preprocessing.ICA(n_components=25, max_iter=1000, random_state=42)\n",
    "    \n",
    "    # applying ICA only to EEG channels only, to filtered raw\n",
    "    ica.fit(raw_eog)\n",
    "    \n",
    "    # Automatically detect bad EOG components using correlation \n",
    "    eog_idx_v, eog_scores_v = ica.find_bads_eog(raw_eog, ch_name='VEOG', measure=\"zscore\", threshold=3)\n",
    "    eog_idx_h, eog_scores_h = ica.find_bads_eog(raw_eog, ch_name='HEOG', measure=\"zscore\", threshold=3)\n",
    "    ica.exclude = eog_idx_v + eog_idx_h \n",
    "    clean_raw = ica.apply(raw_eog.copy())\n",
    "    ica.plot_overlay(raw_eog, exclude = ica.exclude, picks=\"eeg\")\n",
    "\n",
    "    postica = ica.apply(raw_interp)\n",
    "    prep_continuous = postica.filter(l_freq=.1, h_freq=30)\n",
    "    \n",
    "    \n",
    "    # defining epochs of interest \n",
    "    events, event_id = mne.events_from_annotations(prep_continuous)\n",
    "    epochs = mne.Epochs(prep_continuous, events, event_id, tmin=-1, tmax=1.5, baseline= (-.2, 0), preload=True, event_repeated=\"drop\")\n",
    "    \n",
    "    correct_events = [e for e in [\"106\", \"107\", \"116\", \"117\", \"126\", \"127\", \"136\", \"137\"] if e in epochs.event_id]\n",
    "    incorrect_events = [e for e in [\"108\", \"109\", \"118\", \"119\",  \"128\", \"129\", \"138\", \"139\"] if e in epochs.event_id]\n",
    "    event_codes_of_interest = correct_events + incorrect_events\n",
    "    epochs_of_interest = epochs[event_codes_of_interest]\n",
    "    \n",
    "    # applying autoreject on epochs of interest \n",
    "    ar = AutoReject()\n",
    "    epochs_clean = ar.fit_transform(epochs_of_interest)\n",
    "    \n",
    "    # Track number of retained epochs\n",
    "    n_retained_epochs = len(epochs_clean)\n",
    "    n_total_epochs = len(epochs_of_interest)\n",
    "\n",
    "    # Get reject log (based on original epochs)\n",
    "    reject_log = ar.get_reject_log(epochs_of_interest)\n",
    "    removed_indices = np.flatnonzero(reject_log.bad_epochs)   \n",
    "    \n",
    "    # Count incorrect trials per congruency\n",
    "    trial_counts = {}\n",
    "    for label, codes in [('0',['108','109']),('33',['118','119']),\n",
    "                         ('66',['128','129']),('100',['138','139'])]:\n",
    "        valid = [e for e in codes if e in epochs_clean.event_id]\n",
    "        trial_counts[f'n_trials_incorrect_{label}pct'] = len(epochs_clean[valid]) if valid else 0\n",
    "\n",
    "    # Save cleaned epochs to preproc folder\n",
    "    os.makedirs(os.path.dirname(preproc_file), exist_ok=True)\n",
    "    epochs_clean.save(preproc_file, overwrite=True)\n",
    "\n",
    "    # Write summary\n",
    "    summary = {'subject_ID': participant,\n",
    "        'interpolated_channels': bad_channels_raw.info['bads'],\n",
    "        'ica_excluded': np.unique(ica.exclude).tolist(),\n",
    "        'n_total_epochs': n_total_epochs,\n",
    "        'n_retained_epochs': n_retained_epochs,\n",
    "        'removed_epoch_indices': removed_indices.tolist(),\n",
    "        **trial_counts}\n",
    "        \n",
    "    with open(summary_txt_path(participant), 'w') as f:\n",
    "        for k, v in summary.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "         \n",
    "    print(f\"Subject {participant} preprocessed successfully :)!\")\n",
    "\n",
    "    #################\n",
    "    #  PROCESSING   #\n",
    "    #################\n",
    "\n",
    "    # Only include event codes that exist in epochs.event_id\n",
    "    codes = {\n",
    "        'correct': [c for c in [\"106\",\"107\",\"116\",\"117\",\"126\",\"127\",\"136\",\"137\"] if c in epochs_clean.event_id],\n",
    "        'incorrect': [c for c in [\"108\",\"109\",\"118\",\"119\",\"128\",\"129\",\"138\",\"139\"] if c in epochs_clean.event_id]\n",
    "    }\n",
    "    \n",
    "    epochs_corr = epochs_clean[codes['correct']] if codes['correct'] else None\n",
    "    epochs_inc  = epochs_clean[codes['incorrect']] if codes['incorrect'] else None\n",
    "    \n",
    "    ERP_corr = epochs_corr.average() if epochs_corr else None\n",
    "    ERP_inc  = epochs_inc.average()  if epochs_inc  else None\n",
    "    \n",
    "    if ERP_corr: ERP_corr.comment = 'Correct'\n",
    "    if ERP_inc:  ERP_inc.comment  = 'Incorrect'\n",
    "    Diff = None\n",
    "    if ERP_corr and ERP_inc:\n",
    "        Diff = ERP_inc.copy(); Diff._data = ERP_inc._data - ERP_corr._data; Diff.comment='Difference Wave'\n",
    "    \n",
    "    # Save evokeds\n",
    "    ERP_corr.save(evoked_file(participant, 'Correct'), overwrite=True)\n",
    "    ERP_inc.save(evoked_file(participant, 'Incorrect'), overwrite=True)\n",
    "    Diff.save(evoked_file(participant, 'dif'), overwrite=True)\n",
    "    \n",
    "    # Run congruency-specific analysis\n",
    "    plot_dir = paths['plots']\n",
    "    ern_vals = []\n",
    "    for cong in [\"all\",\"0\",\"33\",\"66\",\"100\"]:\n",
    "        ERPc, ERPi, DIF, mean_amp = compute_congruency_ERPs(epochs_clean, cong, participant, plot_dir=plot_dir)\n",
    "        ern_vals.append((cong, mean_amp[0] if mean_amp is not None else np.nan))\n",
    "        \n",
    "    # save ERPs in tsv file \n",
    "    csv_dir = os.path.join(paths['results'], 'ern_values')\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    with open(os.path.join(csv_dir, f\"{participant}_desc-ernvalues.tsv\"), 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow([\"Condition\",\"MeanAmp_µV\"])\n",
    "        writer.writerows(ern_vals)\n",
    "        \n",
    "    print(f\"Processing done :) Saved ERN values for {participant} to {csv_dir}\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
